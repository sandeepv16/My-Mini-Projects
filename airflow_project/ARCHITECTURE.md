# Architecture Documentation

## ğŸ—ï¸ Detailed Architecture Explanation

### 1. System Components

#### Core Services
- **Apache Airflow**: Workflow orchestration and scheduling
- **MySQL**: Transactional data storage
- **PostgreSQL (Airflow)**: Airflow metadata storage
- **PostgreSQL (Metadata)**: Data lineage and metadata tracking

#### Application Layers
- **Presentation Layer**: Airflow Web UI
- **Orchestration Layer**: Airflow Scheduler + Executor
- **Processing Layer**: Python operators with pandas
- **Storage Layer**: MySQL + PostgreSQL databases
- **Data Layer**: File system (input/archive)

---

## ğŸ“Š Data Flow Architecture

### Phase 1: Ingestion
```
CSV File â†’ Scanner â†’ Validator â†’ DataFrame
```
1. Files placed in `data/input/`
2. Scanner detects new CSV files
3. Validator checks structure and content
4. Data loaded into pandas DataFrame

### Phase 2: Parallel Processing
```
DataFrame â†’ â”Œâ”€â†’ MySQL Loader     â†’ MySQL DB
            â””â”€â†’ Metadata Extractor â†’ PostgreSQL DB
```
Two parallel branches:
- **Branch A**: Transform and load data into MySQL
- **Branch B**: Extract metadata and store in PostgreSQL

### Phase 3: Finalization
```
Both Complete â†’ Archive â†’ Log Summary â†’ Complete
```
1. Wait for both parallel tasks
2. Move processed file to archive
3. Log execution summary
4. Mark pipeline as complete

---

## ğŸ”„ Task Dependencies

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  start (EmptyOperator)      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  scan_for_csv_files         â”‚
         â”‚  - List files in input dir  â”‚
         â”‚  - Push path to XCom        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  read_and_validate_csv      â”‚
         â”‚  - Read CSV with pandas     â”‚
         â”‚  - Validate structure       â”‚
         â”‚  - Extract basic metadata   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  load_data_to_mysql     â”‚ â”‚ extract_and_store_      â”‚
â”‚  - Transform data       â”‚ â”‚ metadata                â”‚
â”‚  - Create table         â”‚ â”‚ - Column statistics     â”‚
â”‚  - Batch insert         â”‚ â”‚ - Quality metrics       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  archive_processed_file     â”‚
         â”‚  - Move to archive dir      â”‚
         â”‚  - Add timestamp            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  log_pipeline_summary       â”‚
         â”‚  - Log statistics           â”‚
         â”‚  - Print summary            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  end (EmptyOperator)        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¾ Database Schema

### MySQL Schema (Data Storage)

#### sales_data Table
```sql
CREATE TABLE sales_data (
    id INT AUTO_INCREMENT PRIMARY KEY,
    transaction_id VARCHAR(50),
    customer_id VARCHAR(50),
    product_name VARCHAR(255),
    quantity INT,
    price DECIMAL(10, 2),
    transaction_date DATE,
    region VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_transaction_date(transaction_date),
    INDEX idx_customer_id(customer_id)
);
```

### PostgreSQL Schema (Metadata Storage)

#### file_metadata Table
```sql
CREATE TABLE file_metadata (
    id SERIAL PRIMARY KEY,
    file_name VARCHAR(500) NOT NULL,
    file_path VARCHAR(1000) NOT NULL,
    file_size_bytes BIGINT,
    row_count INTEGER,
    column_count INTEGER,
    columns_info JSONB,
    ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ingestion_status VARCHAR(50),
    target_table VARCHAR(255),
    error_message TEXT,
    processing_duration_seconds DECIMAL(10, 2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### column_metadata Table
```sql
CREATE TABLE column_metadata (
    id SERIAL PRIMARY KEY,
    file_metadata_id INTEGER REFERENCES file_metadata(id),
    column_name VARCHAR(255) NOT NULL,
    column_type VARCHAR(100),
    null_count INTEGER,
    unique_count INTEGER,
    min_value TEXT,
    max_value TEXT,
    sample_values JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### data_quality_metrics Table
```sql
CREATE TABLE data_quality_metrics (
    id SERIAL PRIMARY KEY,
    file_metadata_id INTEGER REFERENCES file_metadata(id),
    metric_name VARCHAR(255) NOT NULL,
    metric_value TEXT,
    metric_type VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

## ğŸ” Security Architecture

### Network Isolation
```
External Network (Port 8080)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow Web Server        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
Internal Docker Network
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       â”‚       â”‚
    â–¼       â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚MySQLâ”‚ â”‚Postgresâ”‚Postgresâ”‚
â”‚3306 â”‚ â”‚5432  â”‚ â”‚5433  â”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
```

### Authentication Layers
1. **Airflow UI**: Username/password (configurable)
2. **MySQL**: User credentials from environment
3. **PostgreSQL**: User credentials from environment
4. **Docker Network**: Isolated bridge network

---

## âš¡ Performance Considerations

### Optimization Strategies

#### 1. Batch Processing
```python
# Insert in batches to optimize memory and network
batch_size = 1000  # Configurable
for i in range(0, total_rows, batch_size):
    batch = df.iloc[i:i+batch_size]
    insert_batch(batch)
```

#### 2. Parallel Execution
```python
# MySQL load and metadata extraction run in parallel
load_mysql >> archive_file
store_metadata >> archive_file
```

#### 3. Database Indexing
```sql
-- Indexes for faster queries
CREATE INDEX idx_transaction_date ON sales_data(transaction_date);
CREATE INDEX idx_ingestion_timestamp ON file_metadata(ingestion_timestamp);
```

#### 4. Connection Pooling
- Each task uses temporary connections
- Connections closed after use
- Prevents connection exhaustion

### Scalability Options

#### Vertical Scaling
- Increase Docker container resources
- Allocate more CPU/memory to databases

#### Horizontal Scaling
- Change executor: LocalExecutor â†’ CeleryExecutor
- Add worker nodes for distributed processing
- Use external databases (AWS RDS, Cloud SQL)

#### Data Volume Scaling
- Adjust batch sizes for large files
- Implement chunked reading for huge CSVs
- Use dynamic task mapping for multiple files

---

## ğŸ”„ Pipeline States

### State Transitions
```
queued â†’ running â†’ success
              â†“
           failed â†’ retry â†’ running â†’ success
                            â†“
                         failed (max retries)
```

### Retry Logic
- **Max Retries**: 2
- **Retry Delay**: 5 minutes
- **Retry Strategy**: Exponential backoff (can be configured)

### Error Handling
1. Task failure â†’ Log error â†’ Update metadata status
2. Retry mechanism kicks in
3. If max retries exceeded â†’ Mark as failed
4. Alert sent (if configured)

---

## ğŸ“ File Management

### Directory Structure
```
data/
â”œâ”€â”€ input/           # New CSV files placed here
â”‚   â””â”€â”€ *.csv       # Automatically detected
â””â”€â”€ archive/         # Processed files moved here
    â””â”€â”€ *_timestamp.csv  # Timestamped for audit trail
```

### File Lifecycle
1. **New**: File placed in `input/`
2. **Processing**: Read by pipeline
3. **Validated**: Structure checked
4. **Loaded**: Data inserted into databases
5. **Archived**: Moved to `archive/` with timestamp
6. **Retained**: Available for audit

---

## ğŸ¯ Trigger Mechanisms

### Manual Trigger
- User clicks play button in UI
- Immediate execution
- Good for: Ad-hoc loads, testing

### Scheduled Trigger
```python
schedule_interval='@daily'      # Every midnight
schedule_interval='@hourly'     # Every hour
schedule_interval='*/15 * * * *' # Every 15 minutes
schedule_interval='0 9 * * 1-5' # Weekdays at 9 AM
```

### External Trigger
```bash
# Via Airflow CLI
airflow dags trigger csv_to_mysql_with_metadata

# Via REST API
curl -X POST http://localhost:8080/api/v1/dags/csv_to_mysql_with_metadata/dagRuns \
  -H "Content-Type: application/json" \
  -u "airflow:airflow" \
  -d '{"conf":{}}'
```

---

## ğŸ“ˆ Monitoring & Observability

### What's Monitored
1. **Task Status**: Success/Failure/Running
2. **Execution Time**: Duration per task
3. **Data Metrics**: Rows processed, file size
4. **Quality Metrics**: Completeness, duplicates
5. **Error Logs**: Stack traces, error messages

### Monitoring Interfaces
- **Airflow UI**: Real-time task monitoring
- **Database Queries**: Historical metadata analysis
- **Docker Logs**: System-level debugging
- **Task Logs**: Detailed execution traces

### Alerting (Can be configured)
- Email on failure
- Slack notifications
- PagerDuty integration
- Custom webhooks

---

## ğŸ”§ Extensibility

### Adding New Data Sources
1. Create new scanner function
2. Add file format parser
3. Update data transformation logic
4. No changes needed in storage layer

### Adding New Destinations
1. Create new connector class
2. Add task in DAG
3. Configure credentials in `.env`
4. Update parallel execution logic

### Adding Data Quality Checks
1. Add validation function in `data_processor.py`
2. Insert as task in DAG
3. Store results in quality metrics table

---

## ğŸ’¡ Best Practices Implemented

âœ… **Separation of Concerns**: Data vs. Metadata storage  
âœ… **Idempotency**: Can re-run without duplicates (add logic if needed)  
âœ… **Error Handling**: Comprehensive try-catch blocks  
âœ… **Logging**: Detailed logs at each step  
âœ… **Configuration Management**: Environment variables  
âœ… **Code Modularity**: Reusable utility functions  
âœ… **Data Lineage**: Track source to destination  
âœ… **Archive Strategy**: Maintain audit trail  
âœ… **Parallel Processing**: Optimize execution time  
âœ… **Batch Operations**: Handle large datasets efficiently  

---

## ğŸ“š Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| Orchestration | Apache Airflow 2.8.0 | Workflow management |
| Language | Python 3.10 | Data processing |
| Data Processing | Pandas 2.1.4 | CSV manipulation |
| Data Storage | MySQL 8.0 | Transactional data |
| Metadata Storage | PostgreSQL 14 | Metadata & lineage |
| Containerization | Docker Compose | Service management |
| Web Server | Flask (Airflow) | UI & API |
| Database Drivers | mysql-connector, psycopg2 | DB connectivity |

---

**This architecture provides a production-ready, scalable, and maintainable data pipeline solution.**
