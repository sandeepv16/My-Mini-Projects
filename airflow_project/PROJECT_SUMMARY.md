# Project Summary: CSV to MySQL Data Pipeline with Metadata Tracking

## âœ… What Has Been Built

A complete, production-ready Apache Airflow data pipeline that:
- **Ingests CSV data** into MySQL database
- **Tracks metadata** in PostgreSQL database
- **Supports manual and automatic triggering**
- **Provides comprehensive monitoring** and logging
- **Includes data quality metrics** and validation
- **Maintains data lineage** and audit trail

---

## ğŸ“¦ Deliverables

### 1. Core Pipeline Components
- âœ… **Airflow DAG**: `dags/csv_ingestion_dag.py` (250+ lines)
- âœ… **Data Processor**: `utils/data_processor.py` (200+ lines)
- âœ… **Database Connectors**: `utils/db_connectors.py` (250+ lines)

### 2. Infrastructure
- âœ… **Docker Compose**: Multi-service orchestration
- âœ… **MySQL Database**: Data storage with initialization scripts
- âœ… **PostgreSQL (x2)**: Airflow metadata + Data lineage
- âœ… **Airflow Services**: Web server + Scheduler + Init

### 3. Configuration
- âœ… **Environment Variables**: `.env` file
- âœ… **MySQL Init**: `config/init-mysql-db.sql`
- âœ… **PostgreSQL Init**: `config/init-metadata-db.sql`
- âœ… **Git Ignore**: `.gitignore`

### 4. Sample Data
- âœ… **Sales CSV**: 50 sample transactions in `data/input/sales_data.csv`

### 5. Documentation
- âœ… **README**: Comprehensive guide (500+ lines)
- âœ… **QUICKSTART**: 5-minute setup guide
- âœ… **ARCHITECTURE**: Detailed technical documentation (400+ lines)
- âœ… **Health Check Script**: `check_setup.py`

---

## ğŸ—ï¸ Architecture Summary

### System Design
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APACHE AIRFLOW                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚Scheduler â”‚  â”‚Web Serverâ”‚  â”‚ Workers  â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚
        â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MySQL DB   â”‚           â”‚ PostgreSQL   â”‚
â”‚              â”‚           â”‚              â”‚
â”‚ â€¢ CSV Data   â”‚           â”‚ â€¢ Metadata   â”‚
â”‚ â€¢ sales_data â”‚           â”‚ â€¢ Lineage    â”‚
â”‚   table      â”‚           â”‚ â€¢ Quality    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow
1. **Input**: CSV files â†’ `data/input/`
2. **Processing**: Airflow DAG tasks (scan â†’ validate â†’ load)
3. **Storage**: MySQL (data) + PostgreSQL (metadata)
4. **Archive**: Processed files â†’ `data/archive/`

---

## ğŸ¯ Key Features

### âœ¨ Core Capabilities
- **Automated CSV Ingestion**: Detects and processes CSV files
- **Parallel Processing**: Data load and metadata extraction run simultaneously
- **Dynamic Schema Creation**: Tables created based on CSV structure
- **Batch Operations**: Efficient handling of large datasets
- **Comprehensive Metadata**: File stats, column profiles, quality metrics
- **Data Lineage**: Track source files to target tables
- **Error Handling**: Retry logic with status tracking
- **Archive Management**: Timestamped file archival

### ğŸ“Š Metadata Tracked
- File information (name, size, path, timestamp)
- Data statistics (row count, column count)
- Column profiling (type, nulls, unique values, min/max)
- Data quality (completeness %, duplicates, memory usage)
- Processing metrics (duration, status, errors)

### ğŸ”„ Trigger Options
- **Manual**: Click button in Airflow UI
- **Scheduled**: Daily, hourly, custom cron expressions
- **External**: CLI, REST API, or other systems

---

## ğŸ“‚ Complete File Structure

```
airflow_project/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ csv_ingestion_dag.py          # Main DAG with 7 tasks
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py                   # Python package init
â”‚   â”œâ”€â”€ data_processor.py             # Data transformation utilities
â”‚   â””â”€â”€ db_connectors.py              # MySQL & PostgreSQL classes
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/                        # Place CSV files here
â”‚   â”‚   â””â”€â”€ sales_data.csv            # Sample data (50 rows)
â”‚   â””â”€â”€ archive/                      # Processed files (auto-populated)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ init-mysql-db.sql             # MySQL table schemas
â”‚   â””â”€â”€ init-metadata-db.sql          # PostgreSQL metadata schemas
â”œâ”€â”€ logs/                             # Airflow execution logs
â”œâ”€â”€ plugins/                          # Custom Airflow plugins
â”œâ”€â”€ docker-compose.yml                # 5 services orchestration
â”œâ”€â”€ .env                              # Environment configuration
â”œâ”€â”€ .gitignore                        # Git exclusions
â”œâ”€â”€ README.md                         # Complete documentation
â”œâ”€â”€ QUICKSTART.md                     # 5-minute setup guide
â”œâ”€â”€ ARCHITECTURE.md                   # Technical deep-dive
â””â”€â”€ check_setup.py                    # Health check script
```

---

## ğŸš€ How to Use

### Quick Start (5 Minutes)

```powershell
# 1. Start services
docker-compose up -d

# 2. Wait 2-3 minutes for initialization

# 3. Open Airflow UI
# Browser: http://localhost:8080
# Login: airflow / airflow

# 4. Enable and trigger the DAG
# Click: csv_to_mysql_with_metadata â†’ Toggle ON â†’ Play button

# 5. Watch execution and verify results
```

### Verify Results

```powershell
# Check MySQL data
docker-compose exec mysql mysql -u data_user -pdata_pass \
  -e "SELECT COUNT(*) FROM data_db.sales_data;"

# Check PostgreSQL metadata
docker-compose exec postgres-metadata psql -U metadata_user metadata_db \
  -c "SELECT * FROM file_metadata;"

# Check archived file
ls data/archive/
```

---

## ğŸ› ï¸ Technology Stack

| Component | Version | Purpose |
|-----------|---------|---------|
| Apache Airflow | 2.8.0 | Workflow orchestration |
| Python | 3.10 | Data processing |
| Pandas | 2.1.4 | CSV manipulation |
| MySQL | 8.0 | Data storage |
| PostgreSQL | 14 | Metadata storage |
| Docker | Latest | Containerization |
| mysql-connector-python | 8.2.0 | MySQL connectivity |
| psycopg2 | 2.9.9 | PostgreSQL connectivity |

---

## ğŸ“Š Pipeline Tasks

### Task Sequence
1. **start**: Initialize pipeline
2. **scan_for_csv_files**: Detect CSV files in input directory
3. **read_and_validate_csv**: Load and validate data
4. **load_data_to_mysql**: Insert data into MySQL (parallel)
5. **extract_and_store_metadata**: Store metadata in PostgreSQL (parallel)
6. **archive_processed_file**: Move to archive with timestamp
7. **log_pipeline_summary**: Log execution statistics
8. **end**: Complete pipeline

### Execution Time
- Small files (<1000 rows): ~30 seconds
- Medium files (1000-10000 rows): ~1-2 minutes
- Large files (10000+ rows): Depends on batch size

---

## ğŸ” Monitoring & Debugging

### Access Points
- **Airflow UI**: http://localhost:8080
- **MySQL**: Port 3306 (user: data_user)
- **PostgreSQL Metadata**: Port 5433 (user: metadata_user)

### Logs
```powershell
# All services
docker-compose logs -f

# Specific service
docker-compose logs -f airflow-scheduler
docker-compose logs -f mysql
docker-compose logs -f postgres-metadata

# Task-specific logs available in Airflow UI
```

### Health Check
```powershell
# Run automated health check
python check_setup.py

# Manual verification
docker-compose ps  # All services should be "Up" and "healthy"
```

---

## ğŸ“ˆ Sample Queries

### MySQL - Analyze Business Data
```sql
-- Total revenue by region
SELECT region, SUM(price * quantity) as revenue
FROM sales_data
GROUP BY region
ORDER BY revenue DESC;

-- Top 10 customers by transaction count
SELECT customer_id, COUNT(*) as transactions
FROM sales_data
GROUP BY customer_id
ORDER BY transactions DESC
LIMIT 10;
```

### PostgreSQL - Analyze Metadata
```sql
-- Ingestion history
SELECT file_name, row_count, ingestion_status, ingestion_timestamp
FROM file_metadata
ORDER BY ingestion_timestamp DESC;

-- Data quality trends
SELECT fm.file_name, dqm.metric_name, dqm.metric_value
FROM data_quality_metrics dqm
JOIN file_metadata fm ON dqm.file_metadata_id = fm.id
WHERE dqm.metric_name = 'data_completeness_percentage';
```

---

## ğŸ“ Learning Outcomes

By using this project, you'll understand:
- âœ… Apache Airflow DAG development
- âœ… Parallel task execution in workflows
- âœ… Database operations (MySQL & PostgreSQL)
- âœ… Data quality and metadata tracking
- âœ… Docker Compose multi-service orchestration
- âœ… Python data processing with pandas
- âœ… Production-ready error handling and logging
- âœ… Data pipeline architecture patterns

---

## ğŸ”§ Customization Options

### Change Schedule
Edit `dags/csv_ingestion_dag.py`:
```python
schedule_interval='@daily'      # Daily
schedule_interval='@hourly'     # Hourly
schedule_interval='*/30 * * * *' # Every 30 minutes
```

### Add More CSV Files
Simply place files in `data/input/` - pipeline auto-detects them.

### Modify Target Table
Edit `TARGET_TABLE` variable in `dags/csv_ingestion_dag.py`.

### Adjust Batch Size
Edit `batch_size` in `utils/db_connectors.py`:
```python
mysql_conn.insert_dataframe(df, table, batch_size=5000)
```

---

## ğŸ‰ Success Criteria

Your pipeline is working correctly if:
- âœ… Airflow UI is accessible at http://localhost:8080
- âœ… All Docker containers show as "healthy"
- âœ… DAG executes without errors
- âœ… MySQL contains ingested data
- âœ… PostgreSQL contains metadata records
- âœ… Files move to archive after processing
- âœ… Logs show successful execution

---

## ğŸ“ Support Resources

- **README.md**: Comprehensive setup and usage guide
- **QUICKSTART.md**: Fast 5-minute getting started
- **ARCHITECTURE.md**: Deep technical documentation
- **Inline Code Comments**: Detailed explanations in all files
- **Airflow Logs**: Real-time execution details
- **Docker Logs**: System-level debugging

---

## ğŸŠ What Makes This Special

### Production-Ready Features
- âœ… **Comprehensive Error Handling**: Try-catch blocks, retries, status tracking
- âœ… **Scalable Design**: Parallel processing, batch operations
- âœ… **Data Governance**: Metadata tracking, lineage, audit trail
- âœ… **Quality Assurance**: Validation, quality metrics, monitoring
- âœ… **Maintainability**: Modular code, clear documentation
- âœ… **Observability**: Detailed logging, monitoring UI, health checks
- âœ… **Security**: Environment variables, network isolation
- âœ… **Extensibility**: Easy to add sources, destinations, transformations

---

## ğŸ† Project Statistics

- **Total Files**: 13 source files
- **Lines of Code**: 1000+ lines (Python, SQL, YAML)
- **Documentation**: 1500+ lines across 3 markdown files
- **Services**: 5 Docker containers
- **Databases**: 3 (Airflow metadata + MySQL + PostgreSQL)
- **Sample Data**: 50 transaction records
- **DAG Tasks**: 8 tasks with parallel execution

---

**ğŸ¯ You now have a fully functional, production-ready data pipeline!**

Start with `QUICKSTART.md` to get up and running in 5 minutes.
